{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Use a pre-trained google/flan-t5-small as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "from transformers import T5ForConditionalGeneration,T5Config\n",
    "from transformers.models.t5.modeling_t5 import T5DenseGatedActDense,T5DenseActDense,T5LayerNorm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text2text-generation\", model=model,tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_summ = \"Summerize : Defending Cricket World Cup champions England's campaign in the 2023 edition went from bad to worse as they slumped to their fourth defeat in five matches. On Thursday, England went down by eight wickets against Sri Lanka in ODI Cricket World Cup match in Bengaluru. With the loss, England went down to the ninth place in the 10-team Cricket World Cup. England have 2 points from five matches (NRR -1.634). Their remaining matches are against India, Australia, Netherlands and Pakistan. With hosts India and five-time champions Australia in good form, England's chances to enter semi-finals are hanging by a thread.\"\n",
    "text_qa = \"question: How much points England have? context:Defending Cricket World Cup champions England's campaign in the 2023 edition went from bad to worse as they slumped to their fourth defeat in five matches. On Thursday, England went down by eight wickets against Sri Lanka in ODI Cricket World Cup match in Bengaluru. With the loss, England went down to the ninth place in the 10-team Cricket World Cup. England have 2 points from five matches (NRR -1.634). Their remaining matches are against India, Australia, Netherlands and Pakistan. With hosts India and five-time champions Australia in good form, England's chances to enter semi-finals are hanging by a thread.\"\n",
    "text_translate =\"Translate to French:  My name is Prabhat and I lives in Hyderabad\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Verify if the summariza'on task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"England's campaign in the 2023 edition went from bad to worse as they slumped\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify if the Q&A task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '2'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify if English to French transla'on task works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"M'ai nom est Prabhat et je vive en Hyderabad\"}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#check_point = \"google/flan-t5-base\"\n",
    "#model = T5ForConditionalGeneration.from_pretrained(check_point)\n",
    "#model_config = model.config.to_dict()\n",
    "\n",
    "#model_config[\"d_model\"] = 128\n",
    "#config = T5Config(**model_config)\n",
    "\n",
    "#model = T5ForConditionalGeneration.from_pretrained(model, config = config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Programma'cally print the names of all the model layers and their dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of layer : shared.weight,'---->' Size of the layer torch.Size([32128, 512])\n",
      "Name of layer : encoder.block.0.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.0.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.0.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.0.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight,'---->' Size of the layer torch.Size([32, 6])\n",
      "Name of layer : encoder.block.0.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.0.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.0.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.0.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.0.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.1.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.1.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.1.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.1.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.1.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.1.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.1.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.1.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.1.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.2.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.2.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.2.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.2.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.2.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.2.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.2.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.2.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.2.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.3.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.3.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.3.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.3.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.3.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.3.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.3.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.3.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.3.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.4.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.4.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.4.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.4.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.4.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.4.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.4.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.4.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.4.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.5.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.5.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.5.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.5.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.5.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.5.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.5.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.5.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.5.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.6.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.6.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.6.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.6.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.6.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.6.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.6.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.6.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.6.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.7.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.7.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.7.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : encoder.block.7.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : encoder.block.7.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.block.7.layer.1.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.7.layer.1.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : encoder.block.7.layer.1.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : encoder.block.7.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : encoder.final_layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.0.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.0.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.0.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.0.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight,'---->' Size of the layer torch.Size([32, 6])\n",
      "Name of layer : decoder.block.0.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.0.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.0.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.0.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.0.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.0.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.0.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.0.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.0.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.0.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.1.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.1.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.1.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.1.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.1.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.1.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.1.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.1.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.1.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.1.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.1.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.1.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.1.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.1.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.2.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.2.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.2.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.2.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.2.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.2.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.2.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.2.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.2.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.2.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.2.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.2.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.2.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.2.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.3.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.3.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.3.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.3.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.3.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.3.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.3.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.3.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.3.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.3.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.3.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.3.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.3.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.3.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.4.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.4.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.4.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.4.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.4.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.4.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.4.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.4.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.4.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.4.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.4.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.4.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.4.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.4.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.5.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.5.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.5.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.5.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.5.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.5.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.5.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.5.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.5.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.5.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.5.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.5.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.5.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.5.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.6.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.6.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.6.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.6.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.6.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.6.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.6.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.6.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.6.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.6.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.6.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.6.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.6.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.6.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.7.layer.0.SelfAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.7.layer.0.SelfAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.7.layer.0.SelfAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.7.layer.0.SelfAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.7.layer.0.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.7.layer.1.EncDecAttention.q.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.7.layer.1.EncDecAttention.k.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.7.layer.1.EncDecAttention.v.weight,'---->' Size of the layer torch.Size([384, 512])\n",
      "Name of layer : decoder.block.7.layer.1.EncDecAttention.o.weight,'---->' Size of the layer torch.Size([512, 384])\n",
      "Name of layer : decoder.block.7.layer.1.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.block.7.layer.2.DenseReluDense.wi_0.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.7.layer.2.DenseReluDense.wi_1.weight,'---->' Size of the layer torch.Size([1024, 512])\n",
      "Name of layer : decoder.block.7.layer.2.DenseReluDense.wo.weight,'---->' Size of the layer torch.Size([512, 1024])\n",
      "Name of layer : decoder.block.7.layer.2.layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : decoder.final_layer_norm.weight,'---->' Size of the layer torch.Size([512])\n",
      "Name of layer : lm_head.weight,'---->' Size of the layer torch.Size([32128, 512])\n"
     ]
    }
   ],
   "source": [
    "for i,j in model.named_parameters():\n",
    "    print(f\"Name of layer : {i},'---->' Size of the layer {j.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Programma'cally print the total number of parameters/weights in this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76961152"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Set the tensor in final layer (decoder.final_layer_norm.weight) to all zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.decoder.final_layer_norm.weight.data = nn.parameter.Parameter(torch.zeros_like(model.decoder.final_layer_norm.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Verify if the Q&A task works aWer reseXng the weights of the above layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': ''}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline(\"text2text-generation\", model=model,tokenizer=tokenizer)\n",
    "text_qa1 = \"question: What is the capital city of India? context: Capital city of India is New Delhi\"\n",
    "pipe(text_qa1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Replace the decoder.final_layer_norm.weight with a layer of smaller dimensions and adjust all the dependent layers to match the dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5LayerFF_128(nn.Module):\n",
    "    def __init__(self, config: T5Config):\n",
    "        super().__init__()\n",
    "        if config.is_gated_act:\n",
    "            self.DenseReluDense = T5DenseGatedActDense(config)\n",
    "        else:\n",
    "            self.DenseReluDense = T5DenseActDense(config)\n",
    "\n",
    "        self.layer_norm = T5LayerNorm(128, eps=config.layer_norm_epsilon)\n",
    "        self.dropout = nn.Dropout(config.dropout_rate)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \n",
    "        hidden_states=hidden_states[:,:,:128]\n",
    "        forwarded_states = self.layer_norm(hidden_states)\n",
    "        forwarded_states = self.DenseReluDense(forwarded_states)\n",
    "        hidden_states = hidden_states + self.dropout(forwarded_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.t5.modeling_t5 import T5LayerNorm,T5LayerFF\n",
    "\n",
    "\n",
    "model.decoder.final_layer_norm = T5LayerNorm(hidden_size=128)\n",
    "\n",
    "## Changed the final_layer_norm to lower dimension of 128 from 512\n",
    "model.decoder.final_layer_norm.weight.data = model.decoder.final_layer_norm.weight.data[0:128]\n",
    "\n",
    "# Below are the adjustment of all other layers\n",
    "model.decoder.block[7].layer[2].layer_norm = T5LayerNorm(hidden_size=128)\n",
    "model.decoder.block[7].layer[2].layer_norm.weight.data = model.decoder.block[7].layer[2].layer_norm.weight.data[0:128]\n",
    "\n",
    "\n",
    "model.lm_head.in_features = 128\n",
    "model.lm_head.in_features = 1024\n",
    "model.lm_head.weight.data = model.lm_head.weight.data[:1024,:128]\n",
    "model.decoder.block[7].layer[2] = T5LayerFF_128(model.config)\n",
    "\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wo.out_features =512\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wo.in_features =128\n",
    "\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wo.weight.data = model.decoder.block[7].layer[2].DenseReluDense.wo.weight.data[:128,:]\n",
    "\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wi_0.out_features =512\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wi_0.weight.data = model.decoder.block[7].layer[2].DenseReluDense.wi_0.weight.data[:,:128]\n",
    "\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wi_1.out_features =512\n",
    "model.decoder.block[7].layer[2].DenseReluDense.wi_1.weight.data = model.decoder.block[7].layer[2].DenseReluDense.wi_1.weight.data[:,:128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'NR 303'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"text2text-generation\", model=model,tokenizer=tokenizer)\n",
    "pipe(text_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'New Sieling Stateling community office office office office office office office office office office office office office'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text_qa1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF_128(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (wo): Linear(in_features=128, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
